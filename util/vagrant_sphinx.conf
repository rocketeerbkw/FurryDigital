#############################################################################
##
## Data source definition
##
#############################################################################

#
# Define a base data source with database connection options.
#
source floofclub
{
        #
        # Database type
        #
        type = mysql

        #
        # Connection info
        #
        sql_host = localhost
        sql_port = 3306
        sql_user = root
        sql_pass =
        sql_db   = fa_sandbox
        # sql_sock = /tmp/mysql.sock

        #
        # MySQL specific client connection flags
        # optional, default is 0
        #
        # mysql_connect_flags = 32 # enable compression

        sql_query_pre = SET SESSION query_cache_type=OFF
}

#
# SUBMISSIONS
# Define a submissions information source.
# Use 'floofclub' source as base and add more options to it.
#
source submissions : floofclub
{
        # Pre-query, executed before the main fetch query.
        # Useful eg. to setup encoding or mark records optional, default is empty.
        # Can be any number of queries.
        #
        # sql_query_pre = SET NAMES utf8
        #
        sql_query_pre = REPLACE INTO _sphinx SELECT 1, MAX(rowid), 'submissions' FROM submissions


        # Post-query, executed on sql_query completion.
        # Optional, default is empty
        #
        # sql_query_post =

        #
        ## Main document fetch query
        #
        # Can specify up to 32 fields.
        # All of the fields which are not document_id or attributes will be full-text indexed.
        #
        # document_id MUST be the very first field
        # document_id MUST be positive (non-zero, non-negative)
        # document_id MUST fit into 32 bits
        # document_id MUST be unique
        #
        # mandatory

        sql_query = \
            SELECT  rowid, \
                    \
                    title, message, keywords, \
                    replace(if(locate('_', url) != 0, substr(substr(url, locate('_', url)+1), 1, locate('.', substr(url, locate('_', url)+1))-1), ''), '_', ' ') as "filename", \
                    u.lower, \
                    \
                    date, user, numtracked, comments, views, width, height, adultsubmission, isscrap, \
                    if(category="7", 3, /**/ if(category="16", 4 , /**/ if(category="13", 5, /**/ if(category="14", 6, /**/ if(category="11", 2, /**/ 1/**/)/**/)/**/)/**/)/**/ ) as "type" \
                    \
            FROM   submissions  AS s LEFT JOIN users AS u ON s.user=u.userid LEFT JOIN user_variables AS uv ON s.user=uv.user_id AND uv.var_id=15 \
            WHERE  (uv.content IS NULL OR uv.content='0') AND rowid<=(SELECT max_doc_id FROM _sphinx WHERE counter_id=1)

        # Range query setup, query that must return min and max ID values
        #
        # sql_query will need to reference $start and $end boundaries if using ranged query:
        #
        #sql_query_range = SELECT MIN(rowid), MAX(rowid) FROM submissions
        #sql_range_step  = 10000

        #
        # Attribute declarations
        #

        # Unsigned integer attribute declaration.
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional.
        # Optional bit size can be specified, default is 32
        #
        # sql_attr_uint = author_id
        # sql_attr_uint = forum_id:9 # 9 bits for forum_id
        # sql_attr_uint = group_id
        #
        sql_attr_uint = user
        sql_attr_uint = numtracked
        sql_attr_uint = comments
        sql_attr_uint = views
        sql_attr_uint = width
        sql_attr_uint = height
        sql_attr_uint = adultsubmission
        sql_attr_uint = type
        sql_attr_uint = date

        # Floating point attribute declaration
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional.
        # Values are stored in single precision, 32-bit IEEE 754 format
        #
        # sql_attr_float = lat_radians
        # sql_attr_float = long_radians

        # Boolean attribute declaration
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional.
        # Equivalent to sql_attr_uint with 1-bit size.
        #
        # sql_attr_bool = is_deleted
        #
        sql_attr_bool = isscrap

        # UNIX timestamp attribute declaration.
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional.
        # Similar to sql_attr_uint, but can also be used with date functions.
        #
        # sql_attr_timestamp = posted_ts
        # sql_attr_timestamp = last_edited_ts
        # sql_attr_timestamp = date_added
        #
        #sql_attr_timestamp = date

        # String ordinal attribute declaration.
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional.
        # Sorts strings (bytewise), and stores their indexes in the sorted list.
        # Sorting by this attr is equivalent to sorting by the original strings.
        #
        # sql_attr_str2ordinal = author_name


        # Multi-valued attribute (MVA) attribute declaration.
        # Multi-value, an arbitrary number of attributes is allowed.
        # Optional
        # MVA values are variable length lists of unsigned 32-bit integers.
        #
        # syntax is ATTR-TYPE ATTR-NAME 'from' SOURCE-TYPE [;QUERY] [;RANGE-QUERY]
        # ATTR-TYPE is 'uint' or 'timestamp'
        # SOURCE-TYPE is 'field', 'query', or 'ranged-query'
        # QUERY is SQL query used to fetch all ( docid, attrvalue ) pairs
        # RANGE-QUERY is SQL query used to fetch min and max ID values, similar to 'sql_query_range'
        #
        # sql_attr_multi = uint tag from query; SELECT id, tag FROM tags
        # sql_attr_multi = uint tag from ranged-query; \
        #       SELECT id, tag FROM tags WHERE id>=$start AND id<=$end; \
        #       SELECT MIN(id), MAX(id) FROM tags

        # Ranged query throttling, in milliseconds.
        # Optional, default is 0 which means no delay.
        # Enforces given delay before each query step
        #
        # sql_ranged_throttle = 0

        # Document info query, ONLY for CLI search (ie. testing and debugging).
        # Optional, default is empty.
        # Must contain $id macro and must fetch the document by that id
        #
        sql_query_info = SELECT rowid, user, title, message FROM submissions WHERE rowid=$id
}

#
# Inherited delta submissions source
#
source submissions_delta : submissions
{
    sql_query_pre = SET NAMES utf8

    sql_query = \
        SELECT  rowid, \
                \
                title, message, keywords, \
                replace(if(locate('_', url) != 0, substr(substr(url, locate('_', url)+1), 1, locate('.', substr(url, locate('_', url)+1))-1), ''), '_', ' ') as "filename", \
                u.lower, \
                \
                date, user, numtracked, comments, views, width, height, adultsubmission, isscrap, \
                if(category="7", 3, /**/ if(category="16", 4 , /**/ if(category="13", 5, /**/ if(category="14", 6, /**/ if(category="11", 2, /**/ 1/**/)/**/)/**/)/**/)/**/ ) as "type" \
                \
        FROM   submissions AS s LEFT JOIN users AS u ON s.user=u.userid LEFT JOIN user_variables uv ON s.user=uv.user_id AND uv.var_id='15' \
        WHERE  (uv.content IS NULL OR uv.content='0') AND rowid>(SELECT max_doc_id FROM _sphinx WHERE counter_id=1)
}

#############################################################################
##
## Index definition
##
#############################################################################

# local index example
#
# this is an index which is stored locally in the filesystem
#
# all indexing-time options (such as morphology and charsets)
# are configured per local index

index submissions
{
        # Document source(s) to index. Multi-value. Mandatory.  Document IDs must be globally unique across all sources
        #
        source = submissions


        # Index files path and file name, without extension. Mandatory. Path must be writable, extensions will be auto-appended.
        #
        path = /etc/sphinxsearch/submissions

        docinfo = extern


        # Memory locking for cached data (.spa and .spi), to prevent swapping. Optional, default is 0 (do not mlock). Requires searchd to be run from root.
        #
        mlock = 0

        # A list of morphology preprocessors to apply
        # Optional, default is empty
        #
        # Builtin preprocessors areL
        # 'none', 'stem_en', 'stem_ru', 'stem_enru', 'soundex', and 'metaphone'
        #
        # Additional preprocessors available from libstemmer are 'libstemmer_XXX', where XXX is algorithm code
        # (see libstemmer_c/libstemmer/modules.txt)
        #
        # morphology    = stem_en, stem_ru, soundex
        # morphology    = libstemmer_german
        # morphology    = libstemmer_sv
        #
        morphology = none

        # Stopword files list (space separated). Optional, default is empty. Contents are plain text, charset_table and stemming are both applied
        #
        # stopwords = /srv/sphinxsearch/stopwords.txt


        # Wordforms file, in "mapfrom > mapto" plain text format. Optional, default is empty.
        #
        # wordforms = /usr/local/etc/sphinx/wordforms.txt


        # Tokenizing exceptions file. Optional, default is empty. Plain text, case sensitive, space insensitive in map-from part. One "Map Several Words => ToASingleOne" entry per line.
        #
        # exceptions = /usr/local/etc/shpinx/exceptions.txt


        # Minimum indexed word length. Default is 1 (index everything).
        #
        min_word_len = 3


        # Charset encoding type. Optional, default is 'sbcs'. Known types are 'sbcs' (Single Byte CharSet) and 'utf-8'
        #
        charset_type = sbcs


        # Minimum prefix length.
        #
        # If prefix length is positive, indexer will not only index all words,
        # but all the possible prefixes (ie. word beginnings) as well.
        #
        # For instance, "exam" query against such index will match documents
        # which contain "example" word, even if they do not contain "exam"
        #
        # Indexing prefixes will make the index grow significantly and could degrade search times.
        #
        # Currently there's no way to rank perfect word matches higher than prefix matches using only one index;
        # You could setup two indexes for that.
        #
        # default is 0, which means NOT to index prefixes
        #
        min_prefix_len = 0


        # Minimum infix length.
        #
        # If infix length is positive, indexer will not only index all words,
        # but all the possible infixes (ie. characters subsequences starting
        # anywhere inside the word) as well.
        #
        # For instance, "amp" query against such index will match documents
        # which contain "example" word, even if they do not contain "amp".
        #
        # Indexing infixes will make the index grow significantly and could degrade search times.
        #
        # Currently there's no way to rank perfect word matches higher than infix matches using only one index;
        # You could setup two indexes for that.
        #
        # Default is 0, which means NOT to index infixes
        #
        min_infix_len = 0


        # List of fields to limit prefix/infix indexing to.
        # Optional, default value is empty (index all fields in prefix/infix mode)
        #
        # prefix_fields = filename
        # infix_fields  = url, domain


        # Enable star-syntax (wildcards) when searching prefix/infix indexes.
        # Known values are 0 and 1.
        # Optional, default is 0 (do not use wildcard syntax)
        #
        # enable_star = 1


        # n-grams length
        #
        # n-grams provide basic CJK support for unsegmented texts. if using
        # n-grams, streams of CJK characters are indexed as n-grams. for example,
        # if incoming stream is ABCDEF and n is 2, this text would be indexed
        # as if it was AB BC CD DE EF.
        #
        # this feature is in alpha version state and only n=1 is currently
        # supported; this is going to be improved.
        #
        # note that if search query is segmented (ie. words are separated with
        # whitespace), words are in quotes and extended matching mode is used,
        # then all matching documents will be returned even if their text was
        # *not* segmented. in the example above, ABCDEF text will be indexed as
        # A B C D E F, and "BCD" query will be transformed to "B C D" (where
        # quotes is phrase matching operator), so the document will match.
        #
        # optional, default is 0, which means NOT to use n-grams
        #
        # ngram_len = 1


        # n-gram characters table
        #
        # specifies what specific characters are subject to n-gram
        # extraction. format is similar to charset_table.
        #
        # optional, default is empty
        #
        # ngrams_chars = U+3000..U+2FA1F


        # Optional, default is empty.
        #
        # phrase_boundary = ., ?, !, U+2026 # horizontal ellipsis


        # Phrase boundary word position increment.
        # Optional, default is 0.
        #
        # phrase_boundary_step = 100


        # Whether to strip HTML tags from incoming documents.
        # Known values are 0 (do not strip) and 1 (do strip)
        # Optional, default is 0
        #
        html_strip = 0


        # What HTML attributes to index if stripping HTML.
        # Optional, default is empty (do not index anything)
        #
        # html_index_attrs = img=alt,title; a=title;


        # What HTML elements contents to strip.
        # Optional, default is empty (do not strip element contents)
        #
        # html_remove_elements  = style, script


        # Whether to preopen index data files on startup.
        # Optional, default is 0 (do not preopen)
        #
        preopen = 1
}

# note how all other settings are copied from main,
# but source and path are overridden (they MUST be)
index submissions_delta : submissions
{
        # Document source(s) to index.
        # Multi-value.
        # Mandatory.
        # Document IDs must be globally unique across all sources
        #
        source = submissions_delta


        # Index files path and file name, without extension.
        # Mandatory.
        # Path must be writable, extensions will be auto-appended.
        #
        path = /etc/sphinxsearch/submissions_delta

}

#############################################################################
##
## indexer settings
##
#############################################################################

indexer
{
        # Memory limit, in bytes, kiloytes (16384K) or megabytes (256M).
        # Optional, default is 32M, max is 2047M, recommended is 256M to 1024M.
        #
        mem_limit = 256M


        # Maximum IO calls per second (for I/O throttling).
        # Optional, default is 0 (unlimited).
        #
        # max_iops = 40


        # Maximum IO call size, bytes (for I/O throttling).
        # Optional, default is 0 (unlimited).
        #
        # max_iosize = 1048576

    # Write buffers are used to write both temporary and final index files when indexing.
    # Larger buffers reduce the number of required disk writes.
    write_buffer = 4M
}

#############################################################################
##
## searchd settings
##
#############################################################################

searchd
{
        # IP address to bind on
        # optional, default is 0.0.0.0 (ie. listen on all interfaces)
        #
        # address = 127.0.0.1
        #
        listen = 0.0.0.0:3312


        # Log file, searchd run info is logged here.
        # Optional, default is 'searchd.log'
        #
        log = /etc/sphinxsearch/searchd.log


        # Query log file, all search queries are logged here.
        # Optional, default is empty (do not log queries)
        #
        #query_log = /srv/sphinxsearch/query.log


        # Client read timeout, seconds.
        # Optional, default is 5
        #
        read_timeout = 5

        # Maximum amount of children to fork (concurrent searches to run).
        # Optional, default is 0 (unlimited)
        #
        max_children = 100


        # PID file, searchd process ID file name.
        # Mandatory
        #
        pid_file = /var/run/sphinxsearch/searchd.pid


        # Max amount of matches the daemon ever keeps in RAM, per-index.
        # WARNING, THERE'S ALSO PER-QUERY LIMIT, SEE SetLimits() API CALL
        # default is 1000 (just like Google)
        #
        max_matches = 3000


        # Seamless rotate, prevents rotate stalls if precaching huge datasets
        # Optional, default is 1
        #
        seamless_rotate = 1


        # Whether to forcibly preopen all indexes on startup
        # Optional, default is 0 (do not preopen)
        #
        preopen_indexes = 0


        # Whether to unlink .old index copies on succesful rotation.
        # Optional, default is 1 (do unlink)
        #
        unlink_old = 1
}
